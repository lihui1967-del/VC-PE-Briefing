import os
import re
import datetime
import requests
import feedparser

# ======================
#  æ•°æ®æºï¼ˆå°½é‡é€‰â€œæŠ•èèµ„å¯†åº¦é«˜ + ç›¸å¯¹ç¨³å®šâ€çš„ï¼‰
#  æ³¨æ„ï¼šRSSHub å…¬å…±å®ä¾‹å¶å°”æ³¢åŠ¨ï¼Œæ‰€ä»¥è¦åšè¯Šæ–­è¾“å‡º
# ======================

CHINA_FEEDS = [
    # 36æ°ªï¼šæŠ•èèµ„ä¸“æ ï¼ˆå¯†åº¦æ›´é«˜ï¼‰
    "https://rsshub.app/36kr/investment",
    # 36æ°ªå¿«è®¯ï¼ˆè¡¥å……ï¼‰
    "https://rsshub.app/36kr/newsflashes",
    # åˆ›ä¸šé‚¦ï¼šæŠ•èèµ„æ ‡ç­¾
    "https://rsshub.app/cyzone/label/æŠ•èèµ„",
    # åˆ›ä¸šé‚¦ï¼šæŠ•èèµ„å‘¨æŠ¥ï¼ˆè¡¥å……å¯†åº¦ï¼‰
    "https://rsshub.app/cyzone/label/æŠ•èèµ„å‘¨æŠ¥",
]

# æµ·å¤–å¯¹æ¯”ï¼ˆå°‘é‡å³å¯ï¼‰
OVERSEAS_FEEDS = [
    "https://techcrunch.com/tag/funding/feed/",
    "https://www.fiercebiotech.com/rss/xml",
]

# ======================
#  Bç­–ç•¥å¼ºè¿‡æ»¤ï¼šå®å¯å°‘ä¹Ÿè¦çœŸèèµ„
# ======================

# åŠ¨ä½œè¯ï¼ˆå¿…é¡»å‡ºç°å…¶ä¸€ï¼‰
ACTION_WORDS = ["å®Œæˆ", "è·", "è·å¾—", "å®£å¸ƒå®Œæˆ", "å®£å¸ƒè·å¾—", "å®Œæˆäº†", "å®Œæˆè¿‘", "å®Œæˆçº¦", "å®Œæˆè¶…"]
# è½®æ¬¡/èèµ„è¯ï¼ˆå¿…é¡»å‡ºç°å…¶ä¸€ï¼‰
ROUND_WORDS = [
    "èèµ„", "å¤©ä½¿è½®", "ç§å­è½®", "Pre-A", "PreA", "Aè½®", "A+è½®", "Bè½®", "Cè½®", "Dè½®", "Eè½®",
    "æˆ˜ç•¥èèµ„", "å¹¶è´­", "æ”¶è´­"
]
# å™ªéŸ³æ’é™¤
NOISE_WORDS = ["è®ºå›", "å³°ä¼š", "æ´»åŠ¨", "ä¼šè®®", "æŠ¥å‘Š", "ç™½çš®ä¹¦", "è§‚ç‚¹", "ç›˜ç‚¹", "é¢„æµ‹", "æ‹›è˜", "å‘å¸ƒä¼š"]

# åŸºé‡‘/å‹Ÿèµ„å¼ºä¿¡å·ï¼ˆæ ‡é¢˜é‡Œå‡ºç°å³å¯ç®—â€œåŸºé‡‘åŠ¨æ€â€ï¼Œä½†ä¹Ÿæ’å™ªéŸ³ï¼‰
FUND_WORDS = ["å‹Ÿèµ„", "å‹Ÿé›†", "é¦–å…³", "ç»ˆå…³", "è®¾ç«‹", "æˆç«‹", "å¤‡æ¡ˆ", "åŸºé‡‘", "GP", "LP"]

# èµ›é“ï¼šç”¨äºåˆ†ç»„ï¼ˆéèµ›é“ä¸€å¾‹å‰”é™¤ï¼Œç¬¦åˆä½ â€œAI/åŒ»ç–—/ç¡¬ç§‘æŠ€/æ¶ˆè´¹/å‰æ²¿ç§‘æŠ€â€çš„è¦æ±‚ï¼‰
SECTOR_RULES = {
    "AI": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "LLM", "AIGC", "å¤šæ¨¡æ€", "ç®—åŠ›", "æœºå™¨äºº", "å…·èº«", "è‡ªåŠ¨é©¾é©¶"],
    "åŒ»ç–—/ç”Ÿç‰©": ["åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "å™¨æ¢°", "IVD", "åŸºå› ", "ç»†èƒ", "æŠ—ä½“", "è‚¿ç˜¤", "è¯Šæ–­", "åˆ¶è¯"],
    "ç¡¬ç§‘æŠ€": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "EDA", "ææ–™", "å…ˆè¿›åˆ¶é€ ", "å·¥ä¸š", "ä¼ æ„Ÿ", "å…‰ç”µ", "å‚¨èƒ½", "ç”µæ± ", "èˆªç©ºèˆªå¤©"],
    "å‰æ²¿ç§‘æŠ€": ["é‡å­", "è„‘æœº", "BCI", "æ ¸èšå˜", "ç©ºé—´", "å«æ˜Ÿ", "åˆæˆç”Ÿç‰©", "æ–°ææ–™", "è¶…å¯¼"],
    "æ¶ˆè´¹": ["æ–°æ¶ˆè´¹", "å“ç‰Œ", "é›¶å”®", "è¿é”", "é¤é¥®", "å’–å•¡", "ç¾å¦†", "æ¯å©´", "æ½®ç©", "å® ç‰©", "é¥®æ–™", "é£Ÿå“"],
}

AMOUNT_RE = re.compile(r"((?:è¶…|è¿‘|çº¦)?\s*\d+(?:\.\d+)?\s*(?:äº¿|ä¸‡)?\s*(?:äººæ°‘å¸|å…ƒ|ç¾å…ƒ|ç¾é‡‘|US\$|USD|RMB)?)", re.I)

def clean(s):
    s = re.sub(r"<[^>]+>", "", s or "")
    return re.sub(r"\s+", " ", s).strip()

def has_any(text, keys):
    t = (text or "").lower()
    return any(k.lower() in t for k in keys)

def detect_sector(text):
    for sector, keys in SECTOR_RULES.items():
        if has_any(text, keys):
            return sector
    return "å…¶ä»–"

def extract_amount(text):
    m = AMOUNT_RE.search((text or "").replace(",", ""))
    return m.group(1).strip() if m else "æœªæŠ«éœ²"

def parse_feed(url, limit=50):
    """
    å…³é”®ï¼šç”¨ requests + timeout å–å†…å®¹ï¼Œé¿å… feedparser ç›´æ¥å¡æ­»æˆ–æ— å£°å¤±è´¥
    è¿”å›ï¼šitems, status
    """
    try:
        r = requests.get(url, timeout=20, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        feed = feedparser.parse(r.content)
        items = []
        for e in feed.entries[:limit]:
            items.append({
                "title": clean(getattr(e, "title", "")),
                "link": getattr(e, "link", ""),
                "summary": clean(getattr(e, "summary", ""))[:220],
            })
        return items, f"OK({len(items)})"
    except Exception as ex:
        return [], f"FAIL({type(ex).__name__}) {ex}"

def is_true_deal(title):
    if not title:
        return False
    if has_any(title, NOISE_WORDS):
        return False
    return has_any(title, ACTION_WORDS) and has_any(title, ROUND_WORDS)

def is_fund_news(title):
    if not title:
        return False
    if has_any(title, NOISE_WORDS):
        return False
    return has_any(title, FUND_WORDS)

def post_to_serverchan(title, desp_md):
    sendkey = os.environ["SENDKEY"]
    api = f"https://sctapi.ftqq.com/{sendkey}.send"
    r = requests.post(api, data={"title": title, "desp": desp_md}, timeout=20)
    r.raise_for_status()

def main():
    today = (datetime.datetime.utcnow() + datetime.timedelta(hours=8)).strftime("%Y-%m-%d")

    try:
        # 1) æŠ“å– + è¯Šæ–­
        diag = []
        all_items = []
        for url in CHINA_FEEDS:
            items, st = parse_feed(url)
            diag.append(f"- {url} -> {st}")
            for it in items:
                it["_src"] = url
            all_items.extend(items)

        # 2) è¿‡æ»¤çœŸèèµ„ï¼ˆå¹¶æŒ‰èµ›é“ï¼‰
        deals = []
        funds = []
        for it in all_items:
            title = it["title"]
            blob = f"{title} {it['summary']}"
            if is_true_deal(title):
                sector = detect_sector(blob)
                if sector != "å…¶ä»–":
                    deals.append({
                        "title": title,
                        "link": it["link"],
                        "sector": sector,
                        "amount": extract_amount(blob),
                        "src": it.get("_src", ""),
                    })
            elif is_fund_news(title):
                # åŸºé‡‘åŠ¨æ€ä¸åšèµ›é“è¦æ±‚
                funds.append({
                    "title": title,
                    "link": it["link"],
                    "amount": extract_amount(blob),
                    "src": it.get("_src", ""),
                })

        # å»é‡ + æ§åˆ¶æ•°é‡ï¼ˆBç­–ç•¥ï¼šå®å¯å°‘ï¼‰
        deals = list({d["title"]: d for d in deals}.values())[:15]
        funds = list({f["title"]: f for f in funds}.values())[:10]

        # 3) æµ·å¤–å¯¹æ¯”ï¼ˆè½»é‡ï¼‰
        overseas = []
        for url in OVERSEAS_FEEDS:
            items, st = parse_feed(url, limit=25)
            # æµ·å¤–åªåšâ€œfundingâ€ç­‰å¼±è¿‡æ»¤
            for it in items:
                blob = (it["title"] + " " + it["summary"]).lower()
                if any(k in blob for k in ["funding", "financing", "raised", "series", "seed", "round"]):
                    overseas.append(it)
        overseas = list({o["title"]: o for o in overseas}.values())[:5]

        # 4) è¾“å‡ºï¼ˆå…³é”®ï¼šå°±ç®— 0 æ¡ï¼Œä¹Ÿè¦æŠŠâ€œæŠ“å–è¯Šæ–­ + åŸå§‹æ ·æœ¬æ ‡é¢˜â€å‘å‡ºæ¥ï¼‰
        md = []
        md.append(f"# {today} VC/PE èèµ„æ™¨æŠ¥ï¼ˆBç­–ç•¥ï¼šå®å¯å°‘ä¹Ÿè¦çœŸï¼‰\n")

        md.append("## âœ… æŠ“å–è¯Šæ–­ï¼ˆéå¸¸é‡è¦ï¼‰")
        md.extend(diag)
        md.append("")

        md.append("## ğŸ‡¨ğŸ‡³ ä¸­å›½çœŸèèµ„ï¼ˆâ‰¤15ï¼‰")
        if deals:
            for i, d in enumerate(deals, 1):
                md.append(f"{i}. **[{d['title']}]({d['link']})**")
                md.append(f"   - èµ›é“ï¼š{d['sector']}ï½œé‡‘é¢ï¼š{d['amount']}")
        else:
            md.append("- ä»Šæ—¥æœªç­›åˆ°â€œæ ‡é¢˜æ˜ç¡®å®Œæˆèèµ„/è½®æ¬¡â€çš„æ¡ç›®ã€‚")
            md.append("- ä¸‹é¢ç»™ä½  10 æ¡åŸå§‹æ ‡é¢˜æ ·æœ¬ï¼ˆç”¨äºåˆ¤æ–­æ˜¯æºé—®é¢˜è¿˜æ˜¯è¿‡æ»¤å¤ªä¸¥ï¼‰ï¼š")
            for i, it in enumerate(all_items[:10], 1):
                md.append(f"  {i}) {it['title']}")

        md.append("\n## ğŸ¦ åŸºé‡‘/å‹Ÿèµ„åŠ¨æ€ï¼ˆâ‰¤10ï¼‰")
        if funds:
            for i, f in enumerate(funds, 1):
                md.append(f"{i}. **[{f['title']}]({f['link']})**")
                md.append(f"   - è§„æ¨¡çº¿ç´¢ï¼š{f['amount']}")
        else:
            md.append("- ä»Šæ—¥æœªç­›åˆ°æ˜ç¡®å‹Ÿèµ„/è®¾ç«‹/å¤‡æ¡ˆç±»æ ‡é¢˜ã€‚")

        md.append("\n## ğŸŒ æµ·å¤–å¯¹æ¯”ï¼ˆâ‰¤5ï¼‰")
        if overseas:
            for o in overseas:
                md.append(f"- **[{o['title']}]({o['link']})**")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æµ·å¤–èèµ„æ¡ç›®ã€‚")

        post_to_serverchan(f"{today} VC/PE æ™¨æŠ¥", "\n".join(md))

    except Exception as ex:
        # å…œåº•ï¼šä»»ä½•å¼‚å¸¸éƒ½è¦å‘åˆ°å¾®ä¿¡
        err_md = f"# {today} æ™¨æŠ¥ç”Ÿæˆå¤±è´¥ï¼ˆå·²æ•è·ï¼‰\n\n- é”™è¯¯ç±»å‹ï¼š{type(ex).__name__}\n- é”™è¯¯ä¿¡æ¯ï¼š{ex}\n\nè¯·åˆ° GitHub Actions æ—¥å¿—æŸ¥çœ‹è¯¦ç»†æŠ¥é”™ã€‚"
        post_to_serverchan(f"{today} æ™¨æŠ¥å¤±è´¥å‘Šè­¦", err_md)

if __name__ == "__main__":
    main()
