import os
import re
import datetime
import requests
import feedparser
from bs4 import BeautifulSoup

# ======================
# é…ç½®åŒº
# ======================

OVERSEAS_FEEDS = [
    "https://techcrunch.com/tag/funding/feed/",
    "https://www.fiercebiotech.com/rss/xml",
]

CHINA_RSS_FEEDS = [
    "https://36kr.com/feed",
    "http://www.tmtpost.com/rss.xml",
    "https://cn.technode.com/feed/",
]

CHINA_HTML_SOURCES = [
    {"name": "æŠ•èµ„ç•Œ-èµ„è®¯", "url": "https://news.pedaily.cn/", "base": "https://news.pedaily.cn"},
]

NOISE_WORDS = [
    "è®ºå›", "å³°ä¼š", "æ´»åŠ¨", "ä¼šè®®", "æŠ¥å‘Š", "ç™½çš®ä¹¦", "è§‚ç‚¹", "ç›˜ç‚¹", "é¢„æµ‹", "æ‹›è˜", "å‘å¸ƒä¼š", "åœ†æ¡Œ", "ç›´æ’­",
    "è®­ç»ƒè¥", "è¯¾ç¨‹", "ç ”è®¨ä¼š", "æ²™é¾™", "æ¦œå•", "å¹´ä¼š", "ä¸“è®¿", "è§£è¯»", "è§‚å¯Ÿ", "è¶‹åŠ¿", "æ–¹æ³•è®º"
]

FUND_WORDS = ["å‹Ÿèµ„", "å‹Ÿé›†", "é¦–å…³", "ç»ˆå…³", "è®¾ç«‹", "æˆç«‹", "å¤‡æ¡ˆ", "åŸºé‡‘", "GP", "LP", "FOF"]

DEAL_ACTION_WORDS = [
    "è·", "å®Œæˆ", "é¢†æŠ•", "è·ŸæŠ•", "åŠ æŒ",
    "æŠ•èµ„", "æ³¨èµ„", "æ”¶è´­", "å¹¶è´­", "æˆ˜ç•¥æŠ•èµ„",
    "å¢èµ„", "å…¥è‚¡", "å‡ºèµ„", "èåˆ°", "èèµ„"
]

SECTOR_RULES = {
    "AI": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "LLM", "AIGC", "å¤šæ¨¡æ€", "ç®—åŠ›", "æœºå™¨äºº", "å…·èº«", "è‡ªåŠ¨é©¾é©¶"],
    "åŒ»ç–—/ç”Ÿç‰©": ["åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "å™¨æ¢°", "IVD", "åŸºå› ", "ç»†èƒ", "æŠ—ä½“", "è‚¿ç˜¤", "è¯Šæ–­", "åˆ¶è¯"],
    "ç¡¬ç§‘æŠ€": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "EDA", "ææ–™", "å…ˆè¿›åˆ¶é€ ", "å·¥ä¸š", "ä¼ æ„Ÿ", "å…‰ç”µ", "å‚¨èƒ½", "ç”µæ± ", "èˆªç©ºèˆªå¤©"],
    "å‰æ²¿ç§‘æŠ€": ["é‡å­", "è„‘æœº", "BCI", "æ ¸èšå˜", "ç©ºé—´", "å«æ˜Ÿ", "åˆæˆç”Ÿç‰©", "æ–°ææ–™", "è¶…å¯¼"],
    "æ¶ˆè´¹": ["æ–°æ¶ˆè´¹", "å“ç‰Œ", "é›¶å”®", "è¿é”", "é¤é¥®", "å’–å•¡", "ç¾å¦†", "æ¯å©´", "æ½®ç©", "å® ç‰©", "é¥®æ–™", "é£Ÿå“"],
}

AMOUNT_RE = re.compile(
    r"((?:è¶…|è¿‘|çº¦)?\s*\d+(?:\.\d+)?\s*(?:äº¿|ä¸‡|ç™¾ä¸‡|åƒä¸‡|åäº¿)?\s*(?:äººæ°‘å¸|å…ƒ|ç¾å…ƒ|ç¾é‡‘|US\$|USD|RMB)?)",
    re.I
)

# ======================
# å·¥å…·å‡½æ•°
# ======================

def clean(s: str) -> str:
    s = re.sub(r"<[^>]+>", "", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def has_any(text: str, keys) -> bool:
    t = (text or "").lower()
    return any(k.lower() in t for k in keys)

def detect_sector(text: str) -> str:
    for sector, keys in SECTOR_RULES.items():
        if has_any(text, keys):
            return sector
    return "å…¶ä»–/å¾…å½’ç±»"

def extract_amount(text: str) -> str:
    m = AMOUNT_RE.search((text or "").replace(",", ""))
    return m.group(1).strip() if m else "æœªæŠ«éœ²"

def is_noise(title: str) -> bool:
    return has_any(title, NOISE_WORDS)

def is_fund_news(title: str) -> bool:
    if not title:
        return False
    if is_noise(title):
        return False
    return has_any(title, FUND_WORDS)

def is_true_deal(title: str) -> bool:
    """
    Bæ¨¡å¼ï¼šVCå®æˆ˜å®½æ¾å…¥æ± 
    - æ’é™¤æ˜æ˜¾å™ªéŸ³
    - æ ‡é¢˜å«åŠ¨ä½œè¯å³å…¥æ± ï¼ˆè·/å®Œæˆ/é¢†æŠ•/æŠ•èµ„/å¹¶è´­/èèµ„...ï¼‰
    """
    if not title:
        return False
    if is_noise(title):
        return False
    return has_any(title, DEAL_ACTION_WORDS)

def fetch_rss(url: str, limit=80):
    try:
        r = requests.get(url, timeout=25, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        d = feedparser.parse(r.content)
        items = []
        for e in d.entries[:limit]:
            items.append({
                "title": clean(getattr(e, "title", "")),
                "link": getattr(e, "link", ""),
                "summary": clean(getattr(e, "summary", ""))[:220],
                "_src": url,
            })
        return items
    except Exception:
        return []

def fetch_html_links(url: str, base: str, limit=220):
    try:
        r = requests.get(url, timeout=25, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        out = []
        for a in soup.find_all("a"):
            title = clean(a.get_text() or "")
            href = a.get("href") or ""
            if not title or len(title) < 6:
                continue

            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = base.rstrip("/") + href
            elif not href.startswith("http"):
                continue

            out.append({"title": title, "link": href, "summary": ""})

        seen = set()
        uniq = []
        for it in out:
            k = (it["title"], it["link"])
            if k not in seen:
                seen.add(k)
                uniq.append(it)

        return uniq[:limit]
    except Exception:
        return []

def post_to_serverchan(title: str, md: str):
    sendkey = os.environ["SENDKEY"]
    api = f"https://sctapi.ftqq.com/{sendkey}.send"
    r = requests.post(api, data={"title": title, "desp": md}, timeout=25)
    r.raise_for_status()

# ======================
# ä¸»é€»è¾‘
# ======================

def main():
    today = (datetime.datetime.utcnow() + datetime.timedelta(hours=8)).strftime("%Y-%m-%d")

    try:
        # 1) ä¸­å›½æ± å­
        pool_cn = []
        for u in CHINA_RSS_FEEDS:
            pool_cn.extend(fetch_rss(u, limit=100))

        for src in CHINA_HTML_SOURCES:
            links = fetch_html_links(src["url"], src["base"], limit=260)
            for it in links:
                it["_src"] = src["name"]
            pool_cn.extend(links)

        # 2) è¿‡æ»¤ï¼šèèµ„å€™é€‰ + åŸºé‡‘åŠ¨æ€
        deals, funds = [], []
        for it in pool_cn:
            title = it.get("title", "")
            blob = f"{title} {it.get('summary','')}"
            if is_true_deal(title):
                deals.append({
                    "title": title,
                    "link": it.get("link", ""),
                    "sector": detect_sector(blob),
                    "amount_hint": extract_amount(blob),
                    "src": it.get("_src", ""),
                })
            elif is_fund_news(title):
                funds.append({
                    "title": title,
                    "link": it.get("link", ""),
                    "amount_hint": extract_amount(blob),
                    "src": it.get("_src", ""),
                })

        # å»é‡ + æ§åˆ¶æ•°é‡
        deals = list({d["title"]: d for d in deals}.values())[:20]
        funds = list({f["title"]: f for f in funds}.values())[:10]

        disclosed = sum(1 for d in deals if d["amount_hint"] != "æœªæŠ«éœ²")
        undisclosed = len(deals) - disclosed

        # 3) æµ·å¤–å¯¹æ¯”
        pool_os = []
        for u in OVERSEAS_FEEDS:
            pool_os.extend(fetch_rss(u, limit=40))

        overseas = []
        for it in pool_os:
            blob = (it["title"] + " " + it.get("summary", "")).lower()
            if any(k in blob for k in ["funding", "financing", "raised", "series", "seed", "round"]):
                overseas.append({"title": it["title"], "link": it["link"]})
        overseas = list({o["title"]: o for o in overseas}.values())[:5]

        # 4) è¾“å‡º
        md = []
        md.append(f"# {today} è‚¡æƒæŠ•èèµ„ Daily Briefingï¼ˆBæ¨¡å¼ï¼‰\n")

        md.append("## ğŸ‡¨ğŸ‡³ ä¸­å›½èèµ„åŠ¨æ€ï¼ˆâ‰¤20ï¼‰")
        if deals:
            for i, d in enumerate(deals, 1):
                md.append(f"{i}. **[{d['title']}]({d['link']})**")
                md.append(f"   - èµ›é“ï¼š{d['sector']}ï½œé‡‘é¢çº¿ç´¢ï¼š{d['amount_hint']}ï½œæ¥æºï¼š{d['src']}")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°èèµ„å€™é€‰æ¡ç›®ã€‚")

        md.append("\n## ğŸ“Š èèµ„è§„æ¨¡ç»Ÿè®¡æ±‡æ€»ï¼ˆç²—ç•¥ï¼‰")
        md.append(f"- ä»Šæ—¥èèµ„å€™é€‰ï¼š**{len(deals)}** æ¡")
        md.append(f"- é‡‘é¢çº¿ç´¢ï¼š**{disclosed}** æ¡ï½œæœªæŠ«éœ²ï¼š**{undisclosed}** æ¡")

        md.append("\n## ğŸ¦ VC/PE åŸºé‡‘åŠ¨æ€ï¼ˆâ‰¤10ï¼‰")
        if funds:
            for i, f in enumerate(funds, 1):
                md.append(f"{i}. **[{f['title']}]({f['link']})**")
                md.append(f"   - è§„æ¨¡çº¿ç´¢ï¼š{f['amount_hint']}ï½œæ¥æºï¼š{f['src']}")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æ˜ç¡®å‹Ÿèµ„/è®¾ç«‹/å¤‡æ¡ˆç±»æ ‡é¢˜ã€‚")

        md.append("\n## ğŸŒ æµ·å¤–å¯¹æ¯”ï¼ˆâ‰¤5ï¼‰")
        if overseas:
            for o in overseas:
                md.append(f"- **[{o['title']}]({o['link']})**")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æµ·å¤–èèµ„æ¡ç›®ã€‚")

        post_to_serverchan(f"{today} è‚¡æƒæŠ•èèµ„æ™¨æŠ¥", "\n".join(md))

    except Exception as ex:
        post_to_serverchan(
            f"{today} æ™¨æŠ¥å¤±è´¥å‘Šè­¦",
            f"# {today} æ™¨æŠ¥ç”Ÿæˆå¤±è´¥ï¼ˆå·²æ•è·ï¼‰\n\n- é”™è¯¯ç±»å‹ï¼š{type(ex).__name__}\n- é”™è¯¯ä¿¡æ¯ï¼š{ex}\n"
        )

if __name__ == "__main__":
    main()
