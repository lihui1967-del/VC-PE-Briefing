import os
import re
import datetime
import requests
import feedparser
from bs4 import BeautifulSoup

# ======================
# é…ç½®åŒº
# ======================

# æµ·å¤– RSSï¼ˆä½ å·²éªŒè¯ OKï¼‰
OVERSEAS_FEEDS = [
    "https://techcrunch.com/tag/funding/feed/",
    "https://www.fiercebiotech.com/rss/xml",
]

# ä¸­å›½ RSSï¼šä½ æˆªå›¾é‡Œ OK çš„
CHINA_RSS_FEEDS = [
    "https://36kr.com/feed",          # OK(30)
    "http://www.tmtpost.com/rss.xml", # OK(20)
    "https://cn.technode.com/feed/",  # OK(10)
]

# ä¸­å›½ HTMLï¼šæŠ•èµ„ç•Œï¼ˆä½ å·² OK(63)ï¼‰
CHINA_HTML_SOURCES = [
    {"name": "æŠ•èµ„ç•Œ-èµ„è®¯", "url": "https://news.pedaily.cn/", "base": "https://news.pedaily.cn"},
]

# å™ªéŸ³æ’é™¤ï¼ˆå‡ºç°å°±å‰”é™¤ï¼‰
NOISE_WORDS = [
    "è®ºå›", "å³°ä¼š", "æ´»åŠ¨", "ä¼šè®®", "æŠ¥å‘Š", "ç™½çš®ä¹¦", "è§‚ç‚¹", "ç›˜ç‚¹", "é¢„æµ‹", "æ‹›è˜", "å‘å¸ƒä¼š", "åœ†æ¡Œ", "ç›´æ’­",
    "è®­ç»ƒè¥", "è¯¾ç¨‹", "ç ”è®¨ä¼š", "æ²™é¾™", "æ¦œå•", "å¹´ä¼š", "ä¸“è®¿", "è§£è¯»", "è§‚å¯Ÿ", "è¶‹åŠ¿", "æ–¹æ³•è®º"
]

# åŸºé‡‘/å‹Ÿèµ„ä¿¡å·ï¼ˆæ ‡é¢˜çº§ï¼‰
FUND_WORDS = ["å‹Ÿèµ„", "å‹Ÿé›†", "é¦–å…³", "ç»ˆå…³", "è®¾ç«‹", "æˆç«‹", "å¤‡æ¡ˆ", "åŸºé‡‘", "GP", "LP", "FOF"]

# VCå®æˆ˜ï¼šèèµ„åŠ¨ä½œè¯ï¼ˆå®½æ¾å…¥æ± ï¼‰
DEAL_ACTION_WORDS = [
    "è·", "å®Œæˆ", "é¢†æŠ•", "è·ŸæŠ•", "åŠ æŒ",
    "æŠ•èµ„", "æ³¨èµ„", "æ”¶è´­", "å¹¶è´­", "æˆ˜ç•¥æŠ•èµ„",
    "å¢èµ„", "å…¥è‚¡", "å‡ºèµ„", "èåˆ°", "èèµ„"
]

# ä½ å…³æ³¨èµ›é“ï¼ˆç”¨äºåˆ†ç»„ï¼‰
SECTOR_RULES = {
    "AI": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "LLM", "AIGC", "å¤šæ¨¡æ€", "ç®—åŠ›", "æœºå™¨äºº", "å…·èº«", "è‡ªåŠ¨é©¾é©¶"],
    "åŒ»ç–—/ç”Ÿç‰©": ["åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "å™¨æ¢°", "IVD", "åŸºå› ", "ç»†èƒ", "æŠ—ä½“", "è‚¿ç˜¤", "è¯Šæ–­", "åˆ¶è¯"],
    "ç¡¬ç§‘æŠ€": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "EDA", "ææ–™", "å…ˆè¿›åˆ¶é€ ", "å·¥ä¸š", "ä¼ æ„Ÿ", "å…‰ç”µ", "å‚¨èƒ½", "ç”µæ± ", "èˆªç©ºèˆªå¤©"],
    "å‰æ²¿ç§‘æŠ€": ["é‡å­", "è„‘æœº", "BCI", "æ ¸èšå˜", "ç©ºé—´", "å«æ˜Ÿ", "åˆæˆç”Ÿç‰©", "æ–°ææ–™", "è¶…å¯¼"],
    "æ¶ˆè´¹": ["æ–°æ¶ˆè´¹", "å“ç‰Œ", "é›¶å”®", "è¿é”", "é¤é¥®", "å’–å•¡", "ç¾å¦†", "æ¯å©´", "æ½®ç©", "å® ç‰©", "é¥®æ–™", "é£Ÿå“"],
}

# é‡‘é¢æå–ï¼ˆç”¨äºå±•ç¤ºï¼Œä¸åšç¡¬è¿‡æ»¤ï¼‰
AMOUNT_RE = re.compile(
    r"((?:è¶…|è¿‘|çº¦)?\s*\d+(?:\.\d+)?\s*(?:äº¿|ä¸‡|ç™¾ä¸‡|åƒä¸‡|åäº¿)?\s*(?:äººæ°‘å¸|å…ƒ|ç¾å…ƒ|ç¾é‡‘|US\$|USD|RMB)?)",
    re.I
)

# ======================
# å·¥å…·å‡½æ•°
# ======================

def clean(s: str) -> str:
    s = re.sub(r"<[^>]+>", "", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def has_any(text: str, keys) -> bool:
    t = (text or "").lower()
    return any(k.lower() in t for k in keys)

def detect_sector(text: str) -> str:
    for sector, keys in SECTOR_RULES.items():
        if has_any(text, keys):
            return sector
    return "å…¶ä»–/å¾…å½’ç±»"

def extract_amount(text: str) -> str:
    m = AMOUNT_RE.search((text or "").replace(",", ""))
    return m.group(1).strip() if m else "æœªæŠ«éœ²"

def is_noise(title: str) -> bool:
    return has_any(title, NOISE_WORDS)

def is_fund_news(title: str) -> bool:
    if not title:
        return False
    if is_noise(title):
        return False
    return has_any(title, FUND_WORDS)

def is_true_deal_vc(title: str) -> bool:
    """
    VCå®æˆ˜å®½æ¾å…¥æ± ï¼š
    - æ’é™¤æ˜æ˜¾å™ªéŸ³
    - åªè¦åŒ…å«â€œè·/å®Œæˆ/é¢†æŠ•/æŠ•èµ„/åŠ æŒ/æ”¶è´­/å¹¶è´­/èèµ„â€ç­‰åŠ¨ä½œè¯ï¼Œå³è®¤ä¸ºæ˜¯èèµ„å€™é€‰
    """
    if not title:
        return False
    if is_noise(title):
        return False
    return has_any(title, DEAL_ACTION_WORDS)

def fetch_rss(url: str, limit=80):
    """RSS ç›´è¿æŠ“å–ï¼šè¿”å› (items, status)"""
    try:
        r = requests.get(url, timeout=25, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        d = feedparser.parse(r.content)
        items = []
        for e in d.entries[:limit]:
            items.append({
                "title": clean(getattr(e, "title", "")),
                "link": getattr(e, "link", ""),
                "summary": clean(getattr(e, "summary", ""))[:220],
            })
        return items, f"OK({len(items)})"
    except Exception as ex:
        return [], f"FAIL({type(ex).__name__}) {ex}"

def fetch_html_links(name: str, url: str, base: str, limit=220):
    """HTML æŠ“å–å…¬å¼€é¡µé¢æ–‡ç« é“¾æ¥ï¼šè¿”å› (links, status)"""
    try:
        r = requests.get(url, timeout=25, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        out = []
        for a in soup.find_all("a"):
            title = clean(a.get_text() or "")
            href = a.get("href") or ""
            if not title or len(title) < 6:
                continue

            # è½¬ç»å¯¹é“¾æ¥
            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = base.rstrip("/") + href
            elif not href.startswith("http"):
                continue

            out.append({"title": title, "link": href})

        # å»é‡
        seen = set()
        uniq = []
        for it in out:
            k = (it["title"], it["link"])
            if k not in seen:
                seen.add(k)
                uniq.append(it)

        return uniq[:limit], f"OK({len(uniq[:limit])})"
    except Exception as ex:
        return [], f"FAIL({type(ex).__name__}) {ex}"

def post_to_serverchan(title: str, md: str):
    sendkey = os.environ["SENDKEY"]
    api = f"https://sctapi.ftqq.com/{sendkey}.send"
    r = requests.post(api, data={"title": title, "desp": md}, timeout=25)
    r.raise_for_status()

# ======================
# ä¸»é€»è¾‘
# ======================

def main():
    today = (datetime.datetime.utcnow() + datetime.timedelta(hours=8)).strftime("%Y-%m-%d")

    try:
        # 1) æŠ“å–ï¼ˆä¸­å›½ï¼‰
        diag_cn = []
        pool_cn = []

        for url in CHINA_RSS_FEEDS:
            items, st = fetch_rss(url, limit=100)
            diag_cn.append(f"- {url} -> {st}")
            for it in items:
                it["_src"] = url
            pool_cn.extend(items)

        for src in CHINA_HTML_SOURCES:
            links, st = fetch_html_links(src["name"], src["url"], src["base"], limit=260)
            diag_cn.append(f"- {src['name']} HTML -> {st} ({src['url']})")
            for it in links:
                it["summary"] = it.get("summary", "")
                it["_src"] = src["name"]
            pool_cn.extend(links)

        # 2) è¿‡æ»¤ï¼šèèµ„å€™é€‰ & åŸºé‡‘åŠ¨æ€
        deals, funds = [], []
        for it in pool_cn:
            title = it.get("title", "")
            blob = f"{title} {it.get('summary','')}"
            if is_true_deal_vc(title):
                deals.append({
                    "title": title,
                    "link": it.get("link", ""),
                    "sector": detect_sector(blob),
                    "amount_hint": extract_amount(blob),
                    "src": it.get("_src", ""),
                })
            elif is_fund_news(title):
                funds.append({
                    "title": title,
                    "link": it.get("link", ""),
                    "amount_hint": extract_amount(blob),
                    "src": it.get("_src", ""),
                })

        # å»é‡ + æ§åˆ¶æ•°é‡ï¼ˆä½ è¦ 20 æ¡å·¦å³ï¼‰
        deals = list({d["title"]: d for d in deals}.values())[:20]
        funds = list({f["title"]: f for f in funds}.values())[:10]

        # 3) æµ·å¤–å¯¹æ¯”
        diag_os = []
        pool_os = []
        for url in OVERSEAS_FEEDS:
            items, st = fetch_rss(url, limit=40)
            diag_os.append(f"- {url} -> {st}")
            pool_os.extend(items)

        overseas = []
        for it in pool_os:
            blob = (it["title"] + " " + it.get("summary", "")).lower()
            if any(k in blob for k in ["funding", "financing", "raised", "series", "seed", "round"]):
                overseas.append({"title": it["title"], "link": it["link"]})
        overseas = list({o["title"]: o for o in overseas}.values())[:5]

        # 4) èèµ„è§„æ¨¡ç»Ÿè®¡ï¼ˆç²—ç•¥ï¼šä»…ç»Ÿè®¡â€œå¯æå–åˆ°é‡‘é¢çº¿ç´¢â€çš„æ¡æ•°ï¼‰
        disclosed = sum(1 for d in deals if d["amount_hint"] != "æœªæŠ«éœ²")
        undisclosed = len(deals) - disclosed

        # 5) è¾“å‡º
        md = []
        md.append(f"# {today} è‚¡æƒæŠ•èèµ„ Daily Briefingï¼ˆBæ¨¡å¼ï¼šVCå®æˆ˜ï¼‰\n")

        md.append("## âœ… æŠ“å–è¯Šæ–­ï¼ˆä¸­å›½æºï¼‰")
        md.extend(diag_cn)
        md.append("")

        md.append("## ğŸ‡¨ğŸ‡³ ä¸­å›½èèµ„åŠ¨æ€ï¼ˆâ‰¤20ï¼‰")
        if deals:
            for i, d in enumerate(deals, 1):
                md.append(f"{i}. **[{d['title']}]({d['link']})**")
                md.append(f"   - èµ›é“ï¼š{d['sector']}ï½œé‡‘é¢çº¿ç´¢ï¼š{d['amount_hint']}ï½œæ¥æºï¼š{d['src']}")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°èèµ„å€™é€‰æ¡ç›®ï¼ˆè‹¥å‡ºç°ï¼Œè¯·æŠŠâ€œæ ‡é¢˜æ ·æœ¬â€å‘æˆ‘ï¼Œæˆ‘ç»§ç»­è¡¥åŠ¨ä½œè¯æˆ–é™å™ªï¼‰ã€‚")

        md.append("\n## ğŸ“Š èèµ„è§„æ¨¡ç»Ÿè®¡æ±‡æ€»ï¼ˆç²—ç•¥ï¼‰")
        md.append(f"- ä»Šæ—¥èèµ„å€™é€‰ï¼š**{len(deals)}** æ¡")
        md.append(f"- é‡‘é¢çº¿ç´¢ï¼š**{disclosed}** æ¡ï½œæœªæŠ«éœ²ï¼š**{undisclosed}** æ¡")

        md.append("\n## ğŸ¦ VC/PE åŸºé‡‘åŠ¨æ€ï¼ˆâ‰¤10ï¼‰")
        if funds:
            for i, f in enumerate(funds, 1):
                md.append(f"{i}. **[{f['title']}]({f['link']})**")
                md.append(f"   - è§„æ¨¡çº¿ç´¢ï¼š{f['amount_hint']}ï½œæ¥æºï¼š{f['src']}")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æ˜ç¡®å‹Ÿèµ„/è®¾ç«‹/å¤‡æ¡ˆç±»æ ‡é¢˜ã€‚")

        md.append("\n## ğŸŒ æµ·å¤–å¯¹æ¯”ï¼ˆâ‰¤5ï¼‰")
        md.append("### æŠ“å–è¯Šæ–­ï¼ˆæµ·å¤–æºï¼‰")
        md.extend(diag_os)
        if overseas:
            md.append("")
            for o in overseas:
                md.append(f"- **[{o['title']}]({o['link']})**")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æµ·å¤–èèµ„æ¡ç›®ã€‚")

        # å…ˆä¸å¼ºè¡Œå†™â€œçƒ­ç‚¹è¶‹åŠ¿ç‚¹è¯„/æ˜æ—¥å…³æ³¨æ¸…å•â€ï¼Œç­‰ä½ è·‘å‡ºç¨³å®šä¸­å›½æ•°æ®åå†åŠ ä¼šæ›´å‡†
        post_to_serverchan(f"{today} è‚¡æƒæŠ•èèµ„æ™¨æŠ¥", "\n".join(md))

    except Exception as ex:
        post_to_serverchan(
            f"{today} æ™¨æŠ¥å¤±è´¥å‘Šè­¦",
            f"# {today} æ™¨æŠ¥ç”Ÿæˆå¤±è´¥ï¼ˆå·²æ•è·ï¼‰\n\n- é”™è¯¯ç±»å‹ï¼š{type(ex).__name__}\n- é”™è¯¯ä¿¡æ¯ï¼š{ex}\n"
        )

if __name__ == "__main__":
    main()
