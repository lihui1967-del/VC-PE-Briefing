import os
import re
import datetime
import requests
import feedparser
from bs4 import BeautifulSoup

# ======================
# é…ç½®åŒº
# ======================

# æµ·å¤– RSSï¼ˆä½ ä¹‹å‰å·²éªŒè¯ OKï¼‰
OVERSEAS_FEEDS = [
    "https://techcrunch.com/tag/funding/feed/",
    "https://www.fiercebiotech.com/rss/xml",
]

# ä¸­å›½ RSSï¼šä½ å›¾é‡ŒéªŒè¯ OK çš„
CHINA_RSS_FEEDS = [
    "https://36kr.com/feed",          # OK(30)
    "http://www.tmtpost.com/rss.xml", # OK(20)
    "https://cn.technode.com/feed/",  # OK(10)
]

# ä¸­å›½ HTMLï¼šç”¨äºâ€œæŠ•èèµ„ä¸“æ /è¡Œä¸šæŠ•èèµ„è®¯â€æŠ“å–ï¼ˆä¸èµ° RSSHubï¼‰
CHINA_HTML_SOURCES = [
    # 36kr æŠ•èèµ„é¢‘é“
    {"name": "36kr-æŠ•èèµ„", "url": "https://36kr.com/investment", "base": "https://36kr.com"},
    # æŠ•èµ„ç•Œï¼ˆPEdailyï¼‰- èµ„è®¯é¦–é¡µï¼ˆå«å¤§é‡æŠ•èèµ„/å‹Ÿèµ„æ ‡é¢˜ï¼‰
    {"name": "æŠ•èµ„ç•Œ-èµ„è®¯", "url": "https://news.pedaily.cn/", "base": "https://news.pedaily.cn"},
]

# Bç­–ç•¥ï¼šæ ‡é¢˜å«â€œèèµ„â€å³ç®—å€™é€‰ï¼ˆä½†ä»æ’å™ªéŸ³ï¼‰
NOISE_WORDS = [
    "è®ºå›", "å³°ä¼š", "æ´»åŠ¨", "ä¼šè®®", "æŠ¥å‘Š", "ç™½çš®ä¹¦", "è§‚ç‚¹", "ç›˜ç‚¹", "é¢„æµ‹", "æ‹›è˜", "å‘å¸ƒä¼š", "åœ†æ¡Œ", "ç›´æ’­",
    "è®­ç»ƒè¥", "è¯¾ç¨‹", "ç ”è®¨ä¼š"
]

# åŸºé‡‘/å‹Ÿèµ„å¼ºä¿¡å·ï¼ˆæ ‡é¢˜çº§ï¼‰
FUND_WORDS = ["å‹Ÿèµ„", "å‹Ÿé›†", "é¦–å…³", "ç»ˆå…³", "è®¾ç«‹", "æˆç«‹", "å¤‡æ¡ˆ", "åŸºé‡‘", "GP", "LP", "FOF"]

# ä½ å…³æ³¨èµ›é“ï¼ˆç”¨äºåˆ†ç»„ï¼›ä¸å‘½ä¸­=ä»å¯æ”¶å½•ï¼Œä½†æ”¾â€œå…¶ä»–/å¾…å½’ç±»â€ï¼‰
SECTOR_RULES = {
    "AI": ["AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "LLM", "AIGC", "å¤šæ¨¡æ€", "ç®—åŠ›", "æœºå™¨äºº", "å…·èº«", "è‡ªåŠ¨é©¾é©¶"],
    "åŒ»ç–—/ç”Ÿç‰©": ["åŒ»ç–—", "åŒ»è¯", "ç”Ÿç‰©", "å™¨æ¢°", "IVD", "åŸºå› ", "ç»†èƒ", "æŠ—ä½“", "è‚¿ç˜¤", "è¯Šæ–­", "åˆ¶è¯"],
    "ç¡¬ç§‘æŠ€": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "EDA", "ææ–™", "å…ˆè¿›åˆ¶é€ ", "å·¥ä¸š", "ä¼ æ„Ÿ", "å…‰ç”µ", "å‚¨èƒ½", "ç”µæ± ", "èˆªç©ºèˆªå¤©"],
    "å‰æ²¿ç§‘æŠ€": ["é‡å­", "è„‘æœº", "BCI", "æ ¸èšå˜", "ç©ºé—´", "å«æ˜Ÿ", "åˆæˆç”Ÿç‰©", "æ–°ææ–™", "è¶…å¯¼"],
    "æ¶ˆè´¹": ["æ–°æ¶ˆè´¹", "å“ç‰Œ", "é›¶å”®", "è¿é”", "é¤é¥®", "å’–å•¡", "ç¾å¦†", "æ¯å©´", "æ½®ç©", "å® ç‰©", "é¥®æ–™", "é£Ÿå“"],
}

AMOUNT_RE = re.compile(r"((?:è¶…|è¿‘|çº¦)?\s*\d+(?:\.\d+)?\s*(?:äº¿|ä¸‡)?\s*(?:äººæ°‘å¸|å…ƒ|ç¾å…ƒ|ç¾é‡‘|US\$|USD|RMB)?)", re.I)


# ======================
# å·¥å…·å‡½æ•°
# ======================

def clean(s: str) -> str:
    s = re.sub(r"<[^>]+>", "", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def has_any(text: str, keys) -> bool:
    t = (text or "").lower()
    return any(k.lower() in t for k in keys)

def detect_sector(text: str) -> str:
    for sector, keys in SECTOR_RULES.items():
        if has_any(text, keys):
            return sector
    return "å…¶ä»–/å¾…å½’ç±»"

def extract_amount(text: str) -> str:
    m = AMOUNT_RE.search((text or "").replace(",", ""))
    return m.group(1).strip() if m else "æœªæŠ«éœ²"

def is_noise(title: str) -> bool:
    return has_any(title, NOISE_WORDS)

def is_true_deal_B(title: str) -> bool:
    """Bç­–ç•¥ï¼šæ ‡é¢˜åŒ…å«â€œèèµ„â€å°±æ”¶ï¼ˆä½†æ’é™¤å™ªéŸ³ï¼‰"""
    if not title:
        return False
    if is_noise(title):
        return False
    return "èèµ„" in title

def is_fund_news(title: str) -> bool:
    if not title:
        return False
    if is_noise(title):
        return False
    return has_any(title, FUND_WORDS)

def fetch_rss(url: str, limit=50):
    """RSS ç›´è¿æŠ“å–ï¼šè¿”å› (items, status)"""
    try:
        r = requests.get(url, timeout=25, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        d = feedparser.parse(r.content)
        items = []
        for e in d.entries[:limit]:
            items.append({
                "title": clean(getattr(e, "title", "")),
                "link": getattr(e, "link", ""),
                "summary": clean(getattr(e, "summary", ""))[:220],
            })
        return items, f"OK({len(items)})"
    except Exception as ex:
        return [], f"FAIL({type(ex).__name__}) {ex}"

def fetch_html_links(name: str, url: str, base: str, limit=120):
    """HTML æŠ“å–å…¬å¼€é¡µé¢çš„æ–‡ç« é“¾æ¥ï¼šè¿”å› (links, status)
       links: list[dict(title, link)]
    """
    try:
        r = requests.get(url, timeout=25, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        out = []
        for a in soup.find_all("a"):
            title = clean(a.get_text() or "")
            href = a.get("href") or ""
            if not title or len(title) < 6:
                continue

            # ç»Ÿä¸€æˆç»å¯¹é“¾æ¥
            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = base.rstrip("/") + href
            elif not href.startswith("http"):
                continue

            out.append({"title": title, "link": href})

        # å»é‡
        seen = set()
        uniq = []
        for it in out:
            k = (it["title"], it["link"])
            if k not in seen:
                seen.add(k)
                uniq.append(it)

        return uniq[:limit], f"OK({len(uniq[:limit])})"
    except Exception as ex:
        return [], f"FAIL({type(ex).__name__}) {ex}"

def post_to_serverchan(title: str, md: str):
    sendkey = os.environ["SENDKEY"]
    api = f"https://sctapi.ftqq.com/{sendkey}.send"
    r = requests.post(api, data={"title": title, "desp": md}, timeout=25)
    r.raise_for_status()


# ======================
# ä¸»é€»è¾‘
# ======================

def main():
    today = (datetime.datetime.utcnow() + datetime.timedelta(hours=8)).strftime("%Y-%m-%d")

    try:
        diag_cn, diag_os = [], []
        pool_cn = []

        # 1) ä¸­å›½ RSS æŠ“å–
        for url in CHINA_RSS_FEEDS:
            items, st = fetch_rss(url, limit=60)
            diag_cn.append(f"- {url} -> {st}")
            for it in items:
                it["_src"] = url
            pool_cn.extend(items)

        # 2) ä¸­å›½ HTML æŠ“å–ï¼ˆæŠ•èèµ„ä¸“æ /æŠ•èèµ„èµ„è®¯ï¼‰
        for src in CHINA_HTML_SOURCES:
            links, st = fetch_html_links(src["name"], src["url"], src["base"], limit=160)
            diag_cn.append(f"- {src['name']} HTML -> {st} ({src['url']})")
            for it in links:
                it["summary"] = it.get("summary", "")
                it["_src"] = src["name"]
            pool_cn.extend(links)

        # 3) è¿‡æ»¤ï¼šèèµ„äº‹ä»¶ & åŸºé‡‘åŠ¨æ€
        deals, funds = [], []
        for it in pool_cn:
            title = it.get("title", "")
            blob = f"{title} {it.get('summary','')}"
            if is_true_deal_B(title):
                deals.append({
                    "title": title,
                    "link": it.get("link", ""),
                    "sector": detect_sector(blob),
                    "amount": extract_amount(blob),
                    "src": it.get("_src", ""),
                })
            elif is_fund_news(title):
                funds.append({
                    "title": title,
                    "link": it.get("link", ""),
                    "amount": extract_amount(blob),
                    "src": it.get("_src", ""),
                })

        # å»é‡ + æ§åˆ¶æ•°é‡
        deals = list({d["title"]: d for d in deals}.values())[:20]  # ä½ è¦ 20 æ¡å·¦å³
        funds = list({f["title"]: f for f in funds}.values())[:10]

        # 4) æµ·å¤–å¯¹æ¯”
        pool_os = []
        for url in OVERSEAS_FEEDS:
            items, st = fetch_rss(url, limit=30)
            diag_os.append(f"- {url} -> {st}")
            pool_os.extend(items)

        overseas = []
        for it in pool_os:
            blob = (it["title"] + " " + it.get("summary", "")).lower()
            if any(k in blob for k in ["funding", "financing", "raised", "series", "seed", "round"]):
                overseas.append({"title": it["title"], "link": it["link"]})
        overseas = list({o["title"]: o for o in overseas}.values())[:5]

        # 5) è¾“å‡º
        md = []
        md.append(f"# {today} è‚¡æƒæŠ•èèµ„ Daily Briefingï¼ˆBç­–ç•¥ï¼šæ ‡é¢˜å«â€œèèµ„â€å³å…¥æ± ï¼‰\n")

        md.append("## âœ… æŠ“å–è¯Šæ–­ï¼ˆä¸­å›½æºï¼‰")
        md.extend(diag_cn)
        md.append("")

        md.append("## ğŸ‡¨ğŸ‡³ ä¸­å›½èèµ„åŠ¨æ€ï¼ˆâ‰¤20ï¼‰")
        if deals:
            for i, d in enumerate(deals, 1):
                md.append(f"{i}. **[{d['title']}]({d['link']})**")
                md.append(f"   - èµ›é“ï¼š{d['sector']}ï½œé‡‘é¢ï¼š{d['amount']}ï½œæ¥æºï¼š{d['src']}")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æ ‡é¢˜å«â€œèèµ„â€çš„æ¡ç›®ï¼ˆæˆ–å‡è¢«å™ªéŸ³è§„åˆ™æ’é™¤ï¼‰ã€‚")

        md.append("\n## ğŸ¦ VC/PE åŸºé‡‘åŠ¨æ€ï¼ˆâ‰¤10ï¼‰")
        if funds:
            for i, f in enumerate(funds, 1):
                md.append(f"{i}. **[{f['title']}]({f['link']})**")
                md.append(f"   - è§„æ¨¡çº¿ç´¢ï¼š{f['amount']}ï½œæ¥æºï¼š{f['src']}")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æ˜ç¡®å‹Ÿèµ„/è®¾ç«‹/å¤‡æ¡ˆç±»æ ‡é¢˜ã€‚")

        md.append("\n## ğŸŒ æµ·å¤–å¯¹æ¯”ï¼ˆâ‰¤5ï¼‰")
        md.append("### æŠ“å–è¯Šæ–­ï¼ˆæµ·å¤–æºï¼‰")
        md.extend(diag_os)
        if overseas:
            md.append("")
            for o in overseas:
                md.append(f"- **[{o['title']}]({o['link']})**")
        else:
            md.append("- ä»Šæ—¥æœªæŠ“åˆ°æµ·å¤–èèµ„æ¡ç›®ï¼ˆæˆ–å½“å¤© funding æ–‡ç« è¾ƒå°‘ï¼‰ã€‚")

        post_to_serverchan(f"{today} è‚¡æƒæŠ•èèµ„æ™¨æŠ¥", "\n".join(md))

    except Exception as ex:
        post_to_serverchan(
            f"{today} æ™¨æŠ¥å¤±è´¥å‘Šè­¦",
            f"# {today} æ™¨æŠ¥ç”Ÿæˆå¤±è´¥ï¼ˆå·²æ•è·ï¼‰\n\n- é”™è¯¯ç±»å‹ï¼š{type(ex).__name__}\n- é”™è¯¯ä¿¡æ¯ï¼š{ex}\n"
        )

if __name__ == "__main__":
    main()
